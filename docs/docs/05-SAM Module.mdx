import VideoPlayer from "@site/src/components/VideoPlayer";

The SAM module provides segmentation assistance through the [Segment Anything Model](https://segment-anything.com/) by Meta AI.
This document delves into its advanced usage. If you're getting started, consult the [quick start](/Quick%20Start#sam-module) for basic setup.

The SAM module includes the following commands. For a detailed overview, run `nutsh-sam -h`:

- `start`: Start the server.
- `quantize`: Quantize a pre-trained decoder.
- `finetune`: Fine-tune a pre-trained decoder;
- `requirements`: Display the runtime Python requirements.
- `eject`: Extract the Python scripts for additional customization.

## Multiple GPUs Support

Use the `--devices` flag with the `nutsh-sam start` command to designate the list of devices for running the encoder for image embedding.
Specify devices by providing comma-separated device names. Ensure the device names are in the PyTorch format (e.g. `cuda:1`, `cpu`, `mps`).
An integer `n` will be automatically converted to `cuda:{n}`.

For example, the following setting will run on `cuda:0` and `cuda:1`.

```bash
nutsh-sam start --devices 0,1 # ... other flags
```

Furthermore, device names can be duplicated if your GPU is large enough to fit more than one SAM.

```bash
nutsh-sam start --devices 0,0,1,1 # ... other flags
```

Embedding requests will be executed sequentially on each device, and balanced across different devices.

## SAM Decoder Fine-tuning

Sometimes the predictions generated by SAM may not meet our expectations. In such instances, we can adjust the predictions in the browser.
The modifications made are collected and can be utilized to fine-tune a new SAM decoder, tailoring it to our specific requirements.
These modifications are saved as:

```
${WORKSPACE}/sample/project_${PID}/*.json
```

Here, `WORKSPACE` refers to the workspace of the `nutsh` server, and `PID` denotes the project's ID, which can be found in the browser's URL.

:::note

The JSON schema is defined through Protocol Buffers as `Sample` in [proto/definition/schema/v1/train.proto](https://github.com/SysCV/nutsh/blob/main/proto/definition/schema/v1/train.proto).

:::

Once you have accumulated enough samples, the next step is to fine-tune a new SAM decoder.
Begin by ensuring you've followed the [quick start](/Quick%20Start#sam-module) guidelines to activate a virtual environment loaded with the required dependencies.
Afterwards, execute the following command:

```bash
(nutsh-sam) nutsh-sam finetune \
    # path to the directory containing sample JSONs
    --sample-dir ${SAMPLE_DIR} \
    # workspace dedicated to the SAM module, separate from the core's workspace
    --workspace ${SAM_WORKSPACE} \
    # model type, for instance `vit_h`
    --model-type ${MODEL_TYPE} \
    # checkpoint to be fine-tuned
    --model-checkpoint ${MODEL_CHECKPOINT} \
    # device designated for fine-tuning
    --device ${DEVICE} \
    # path to save the newly fine-tuned quantized decoder in ONNX format
    --decoder-path ${DECODER_PATH}
```

The `nutsh-sam finetune` command also incorporates flags allowing adjustments to basic hyperparameters,
such as the learning rate `--lr` or weight decay `--wd`.
For a comprehensive list of options, refer to `nutsh-sam finetune -h`.

Upon successful completion of the process, a newly fine-tuned quantized decoder in ONNX format will be stored at `DECODER_PATH`.
This decoder can subsequently be utilized by the nutsh-sam start command for serving.

<VideoPlayer url="https://nutsh-public.s3.eu-central-1.amazonaws.com/doc/video/finetune_sam.mp4" />

## Customization

Under the hood, the SAM module simply runs a series of Python scripts.
If the default behavior doesn't meet your needs, you have the option to export all these scripts to a directory.
This allows for deeper customization and adjustments as per your requirements.

```bash
nutsh-sam eject --dir ${DIR}
```
